This utiity uses Reed-Solomon coding to provide fault tolerant data storage.

This program is based on two papers, 
"H. Peter Anvin, 'The mathematics of RAID-6', Decemer 2004"
and
"James S. Plank, ' A Tutorial on Reed-Solomon Coding for Fault-Tolerance in 
RAID-like Systems' (http://www.cs.utk.edu/~plank/plank/papers/CS-96-332.html)".

The idea is to take a stream of data and store it in a number of files
while calculating and adding in parity data.

These files can then be transported by potentially unreliable means.

Provided sufficient files arrive intact the original data stream can 
be fully recovered.

split CriticalData into 10 data files with 5 parity files
$ gfm crit. 10 5 < CriticalData 

the created files:
$ ls
CriticalData  crit.10  crit.13  crit.3  crit.6  crit.9
crit.0        crit.11  crit.14  crit.4  crit.7  crit.md5
crit.1        crit.12  crit.2   crit.5  crit.8

The first 10 files contain the original data, slightly mangled and 
with some housekeeping data added. These are the 'data' files.
The last 5 files contain the calculated parity data (plus housekeeping).
These are the 'parity' files. 

In practice 'data' and 'parity' files are interchangeable when it 
comes to data recovery.

The md5 file contains the MD5 checksumsn of the original data 
and the created files:

$ md5sum --check crit.md5  < CriticalData 
crit.0: OK
crit.1: OK
crit.2: OK
crit.3: OK
crit.4: OK
crit.5: OK
crit.6: OK
crit.7: OK
crit.8: OK
crit.9: OK
crit.10: OK
crit.11: OK
crit.12: OK
crit.13: OK
crit.14: OK
-: OK

Note that the 'data' files, being slightly rearranged versions 
of the raw data, have the same 'compressability' as the original 
data. The 'parity' files tend not to compress much as they're made 
up of all the data files mixed together. Compress before you gfm!

Now assume that only 10 of these files made it:
$ ls
CriticalData  crit.10  crit.13  crit.2  crit.5  crit.8
crit.1        crit.11  crit.14  crit.4  crit.7  crit.md5

To recover the data from the remaining files 
(checking MD5 checksums as we go):
$ gfm crit. | tee CriticalData.recovered | md5sum --check crit.md5 
md5sum: crit.0: No such file or directory
crit.0: FAILED open or read
crit.1: OK
crit.2: OK
md5sum: crit.3: No such file or directory
crit.3: FAILED open or read
crit.4: OK
crit.5: OK
md5sum: crit.6: No such file or directory
crit.6: FAILED open or read
crit.7: OK
crit.8: OK
md5sum: crit.9: No such file or directory
crit.9: FAILED open or read
crit.10: OK
crit.11: OK
md5sum: crit.12: No such file or directory
crit.12: FAILED open or read
crit.13: OK
crit.14: OK
-: OK
md5sum: WARNING: 5 of 16 listed files could not be read

And verify:
$ diff CriticalData CriticalData.recovered && echo OK
OK

Notes
=====
Files, if present, are assumed to be correct. Depending on the 
transport mechanism it may be advisable to verify this.

The total number of files (data + parity) must be less tan or 
equal to 250.

The maximum number of files that can be lost without loss of data is 
equal to the number of parity files generated.

The above examples are obviously just a start:

# create a secret password file. You may like to pick a better password :-)
$ date > /home/sprinkmeier/MySecretPassword

# tar up the ~/src directory, encrypt it and split it up with parity:
$ tar --create --file - --directory ~ src \
    | gpg --passphrase-file ~/MySecretPassword --batch --symmetric \
    | gfm mailme. 10 5

# result:
$ ls
mailme.0   mailme.11  mailme.14  mailme.4  mailme.7  mailme.md5
mailme.1   mailme.12  mailme.2   mailme.5  mailme.8
mailme.10  mailme.13  mailme.3   mailme.6  mailme.9

# now email the lot:
$ for X in * ; do uuencode "$X" "$X" | mail someone@example.com -s "$X" ; done

# recover:
$ gfm mailme. \
    |  gpg --passphrase-file ~/MySecretPassword --batch --quiet \
    > src.tar
gpg: WARNING: message was not integrity protected


VIRAL
=====
What's the use of data without a recovery tool?

Tarballs have no redundancy, but at least everyone has 
tar to extract the data.

$ tar czf - -C /var/cache/apt/archives/ . | gfm deb.tar.gz 5 5
tar: ./lock: Cannot open: Permission denied
tar: Error exit delayed from previous errors
$ ls
deb.tar.gz.0  deb.tar.gz.2  deb.tar.gz.4  deb.tar.gz.6  deb.tar.gz.8
deb.tar.gz.1  deb.tar.gz.3  deb.tar.gz.5  deb.tar.gz.7  deb.tar.gz.9
 
OK, so you have a bunch of data files, but no recovery tool. Not so...

Every file has a tarball of the recovery tool prepended, along 
with a reminder of how to extract it:

$ head deb.tar.gz.5
dd bs=64 skip=1 < FILE | tar xjf -



$ dd bs=64 skip=1 < deb.tar.gz.3 | tar xjf -

bzip2: (stdin): trailing garbage after EOF ignored
tar: Child died with signal 13
tar: Error exit delayed from previous errors

Don't worry about the error, that's just bunzip complaining about the 
extra 'junk' at the end of the 'tarball'

$ ls
deb.tar.gz.0  deb.tar.gz.3  deb.tar.gz.6  deb.tar.gz.9
deb.tar.gz.1  deb.tar.gz.4  deb.tar.gz.7  deb.tar.gz.md5
deb.tar.gz.2  deb.tar.gz.5  deb.tar.gz.8  gpl3_src-12
$ cd gpl3_src-12/
$ ls
Makefile  gfa.hh  gfm.README  gfm.cc  gpl3_src.svndump

This is all you need to recreate the gfm tool, and more:

$ make
if [ -x ../misc/svn_export.pl ] ; then ../misc/svn_export.pl --output tmp ; else ln -s `which touch` tmp ; fi
echo 'unsigned char tar[] = {};' > tmp.h
./tmp tmp.h
g++  -lssl -Wall -Werror -DREAL_TAR_ARRY gfm.cc gfa.hh tmp.h -o gfm
$ ls
Makefile  gfa.hh  gfm  gfm.README  gfm.cc  gpl3_src.svndump  tmp  tmp.h

But does it blend^Wunblend?

$ ./gfm ../deb.tar.gz. | tar tzvf - | head
drwxr-xr-x root/root         0 2008-04-18 22:00 ./
-rw-r--r-- root/root   9197358 2008-03-27 00:34 ./firefox_2.0.0.13+1nobinonly-0ubuntu0.7.10_i386.deb
-rw-r--r-- root/root  17804030 2007-01-23 04:34 ./boson-data_0.13-1_all.deb
-rw-r--r-- root/root     48236 2008-03-20 21:34 ./mysql-client_5.0.45-1ubuntu3.3_all.deb
-rw-r--r-- root/root     84214 2007-07-25 21:33 ./libgnome32_1.4.2-36_i386.deb
-rw-r--r-- root/root   1251810 2007-07-30 18:33 ./manpages-dev_2.62-1_all.deb
-rw-r--r-- root/root    160860 2007-11-14 07:34 ./emacs22-bin-common_22.1-0ubuntu5.1_i386.deb
-rw-r--r-- root/root     79386 2007-10-02 23:34 ./libjpeg-progs_6b-14_i386.deb
-rw-r--r-- root/root    884762 2008-01-19 04:34 ./update-manager_1%3a0.81.2_all.deb
-rw-r--r-- root/root     29256 2008-02-20 23:34 ./libavahi-glib1_0.6.20-2ubuntu3.3_i386.deb

But how do you rebuild it 'properly', i.e. so that the data files created 
by the new GFM also include the makings of GFM?

$ mkdir .SVN
$ svnadmin create .SVN/gpl3_src
$ svnadmin load .SVN/gpl3_src < gpl3_src.svndump 
<<< Started new transaction, based on original revision 1
[...]
------- Committed revision 12 >>>

$ svn checkout file://`pwd`/.SVN/gpl3_src
A    gpl3_src/trunk
[...]
Checked out revision 12.
$ cd gpl3_src/trunk/gfm/
$ make
svn_export.pl --output tmp
[...]
g++  -lssl -Wall -Werror -DREAL_TAR_ARRY gfm.cc gfa.hh tmp.h -o gfm

Done!